---
title: "mortality_model_dataset"
output: html_document
date: "2024-06-24"
---

## Load data and libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/mhwhi/Downloads/uscrs_2.0")
load("discrete_final_data.RData")
load("df_intervals_setup.RData")
library(dplyr)
library(splines)
library(tidyr)
library(ggplot2)

# recode outcome as 0 if patient received transplant
## want outcome to be an indicator of pretx death
df_final = df_final %>%
  group_by(PX_ID) %>%
  mutate(any_tx=any(transplant==1)) %>%
  ungroup() %>%
  mutate(outcome = ifelse(any_tx,0,outcome))
df_final %>% filter(outcome==1 | !is.na(time_until_death_after_delisting)) %>% distinct(PX_ID) %>% nrow() # 1311 - yay!

```

# Split data into training / testing sets
```{r}
set.seed(123)
center_ids_train <- sample(x = unique(df_final$CAN_LISTING_CTR_CD), 
                           size = ceiling(0.7*length(unique(df_final$CAN_LISTING_CTR_CD))), 
                           replace = F)
df_final$train_test <-  'Test'
df_final$train_test[df_final$CAN_LISTING_CTR_CD %in% center_ids_train] <- 'Train'

train_data = df_final %>% filter(train_test == 'Train')
test_data = df_final %>% filter(train_test == 'Test')
train_data

save(test_data, file = "mortality_model_test_data.RData")
```


## Create week variable
```{r}
train_data = train_data %>% 
  mutate(
  week = interval_start / 7 + 1
) %>% relocate(week, .after = CAN_LISTING_CTR_CD)

train_data %>% select(PX_ID,week,interval_start,interval_stop,transplant,outcome,time_until_censor_after_delisting,time_until_death_after_delisting) %>% head(15)

```


## Adjust delisting time variables
```{r}

# Censoring at 6 weeks
# Depending on whether the week is even or odd, this translates to 4 or 5 weeks of follow up after delisting
# If candidates dies more than 4/5 weeks after delisting, they are censored
train_data = train_data %>% group_by(PX_ID) %>% 
  
  mutate(
    # if we have full follow-up for the week, follow for censoring (floor)
    time_until_censor_after_delisting = 
      case_when(row_number() == n() & !is.na(time_until_censor_after_delisting) ~ 
                  floor(pmax(0,removal_time + time_until_censor_after_delisting - interval_stop)/7)*7,
                T ~ NA),
    # follow for death even w/o full week (ceiling)
    time_until_death_after_delisting = 
      case_when(row_number() == n() & !is.na(time_until_death_after_delisting) ~ 
                  ceiling(pmax(0,removal_time + time_until_death_after_delisting - interval_stop)/7)*7,
                T ~ NA)) %>%
  
  fill(time_until_censor_after_delisting,time_until_death_after_delisting,.direction="downup") %>%
  
  # censor at 6 weeks
  mutate(time_until_censor_after_delisting = case_when(
    time_until_censor_after_delisting > 28 & max(week) %% 2 == 0 ~ 28,
    time_until_censor_after_delisting > 35 & max(week) %% 2 != 0 ~ 35,
    time_until_death_after_delisting > 28 & max(week) %% 2 == 0 ~ 28,
    time_until_death_after_delisting > 35 & max(week) %% 2 != 0 ~ 35,
    TRUE ~ time_until_censor_after_delisting
  )
)

## Determine delisting row and # of duplicates we need after delisting, until death or censoring

# Delisting is last row in PX_ID with non-na values for time after delisting
# Max duplicates after delisting within a landmark is determined by these time variables
train_data = train_data %>% group_by(PX_ID) %>% mutate(
  delisting = case_when(
    !is.na(time_until_death_after_delisting) & row_number() == n() & n() %% 2 == 0 & time_until_death_after_delisting <= 28 ~ 1,
     !is.na(time_until_death_after_delisting) & row_number() == n() & n() %% 2 != 0 & time_until_death_after_delisting <= 35 ~ 1,
    !is.na(time_until_censor_after_delisting) & row_number() == n() ~ 1,
    TRUE ~ 0)
  ) %>% mutate(
  duplicates_after_delisting = case_when(
    delisting == 1 & time_until_death_after_delisting > 0 & is.na(time_until_censor_after_delisting) ~
      time_until_death_after_delisting/7,
    delisting == 1 & time_until_censor_after_delisting > 0 ~ 
      time_until_censor_after_delisting/7,
  TRUE ~ 0))

# repeat rows for the duplicates after delisting and recode week, intervals
train_data = train_data %>%
  ungroup() %>%
  slice(rep(row_number(), duplicates_after_delisting+1)) %>%
  group_by(PX_ID) %>%
  mutate(week=row_number()) %>%
  group_by(PX_ID,duplicates_after_delisting) %>%
  mutate(dup=row_number()-1) %>%
  ungroup() %>%
  mutate(interval_start=ifelse(duplicates_after_delisting>0,interval_start+7*dup,interval_start),
         interval_stop=ifelse(duplicates_after_delisting>0,interval_stop+7*dup,interval_stop)) %>%
  select(-dup)

# code outcome for last row
train_data = train_data %>%
  group_by(PX_ID) %>%
  mutate(outcome = case_when(
    row_number()==n() & outcome==1 ~ 1,
    row_number()==n() & !is.na(time_until_death_after_delisting) & is.na(time_until_censor_after_delisting) ~ 1,
    T ~ 0))
table(train_data$outcome)

# more than one death per px_id? no
train_data %>% group_by(PX_ID) %>% filter(sum(outcome)>1)
  
# more than one transplant per px_id? no
train_data %>% group_by(PX_ID) %>% filter(sum(transplant)>1)

# transplant or death not on last row? no
train_data %>% filter(outcome==1,row_number()!=n())
train_data %>% filter(transplant==1,row_number()!=n())

# # Total duplicates after delisting as a function of week and the previous delisting variable
# train_data = train_data %>% group_by(PX_ID) %>% mutate(
#   total_duplicates = case_when(
#     week <= 2 ~ duplicates_after_delisting,
#     week == 3 & duplicates_after_delisting %in% c(1,2,3) ~ 2*duplicates_after_delisting,
#     week == 3 & duplicates_after_delisting == 4 ~ 7,
#     week == 3 & duplicates_after_delisting == 5 ~ 8,
#     week %% 2 == 0 & duplicates_after_delisting %in% c(1,2) ~ 2*duplicates_after_delisting,
#     week %% 2 == 0 & duplicates_after_delisting == 3 ~ 5,
#     week %% 2 == 0 & duplicates_after_delisting == 4 ~ 6,
#     week %% 2 != 0 & duplicates_after_delisting %in% c(1,2,3) ~ 2*duplicates_after_delisting + 1,
#     week %% 2 != 0 & duplicates_after_delisting == 4 ~ 8,
#     week %% 2 != 0 & duplicates_after_delisting == 5 ~ 9,
#     TRUE ~ 0
#   )
# )
```

```{r}
# Remove total_duplicates from train_data so columns match
train_data = train_data %>% select(-any_tx,-duplicates_after_delisting)
```

# Fit transplant model, create probability of no transplant variable
```{r}

# fitting transplant model to entire dataset
tx_status_hemo_model = glm(formula = as.formula(
   transplant ~ 
    ns(albumin, df = 3) + ns(bilirubin, df = 3) + ns(eGFR, df = 3) + ns(sodium, df = 3) + 
    ns(BNP, df = 3)*BNP_NT_Pro + 
    durable_LVAD + short_MCS_ever + status + ns(cpo, df = 3) + ns(api, df = 3) + ns(papi, df = 3)), data = df_final, family = binomial)

train_data = train_data %>% ungroup() %>% mutate(
  prob_no_tx = 1 - predict(tx_status_hemo_model, newdata = train_data, type = "response"))


```

## Adjust transplant and prob_no_tx variables
```{r}
# set prob_no_tx to 1 after delisting
train_data = train_data %>%
  group_by(PX_ID) %>%
  mutate(delisting2 = cumsum(delisting),
         prob_no_tx = ifelse(delisting2>1,1,prob_no_tx))

train_data %>% filter(PX_ID==1251148) %>% select(PX_ID,week,interval_start,interval_stop,transplant,outcome,delisting,delisting2,prob_no_tx) %>% tail()

# delisting_duplicates = delisting_duplicates %>% mutate(
#   transplant = 0,
#   prob_no_tx = 1 
# )

# combined_df = rbind(train_data, delisting_duplicates) %>% select(-duplicate_amount)
```



# Create duplicate observations for each landmark
```{r}
# save current version, without duplicates for landmarks
train_data_short = train_data

# First 4 weeks don't have 3 landmarks, so they have fewer duplicates
train_data = train_data %>% 
  ungroup() %>%
  mutate(
  duplicate_amount = case_when(
    week %in% c(1,2) ~ 0,
    week %in% c(3,4) ~ 1,
    TRUE ~ 2
  ))
  
train_data = train_data %>%  # Ensure duplicate_amount is at least 1
  slice(rep(row_number(), duplicate_amount+1)) %>% relocate(duplicate_amount, .after = CAN_LISTING_CTR_CD)
```

## Make separate delisting duplicate dataset
```{r}

# # Filter to only observations where there are >0 duplicates after delisting
# # Duplicate these entries based on that amount, set most variables to NA
# delisting_duplicates = train_data %>%
#   ungroup() %>%
#   filter(total_duplicates > 0) %>%
#   uncount(total_duplicates) %>%
#   mutate(across(!PX_ID & !week & !duplicates_after_delisting & !outcome, ~ NA))
# 
# # Adjust week variable as a function of the original week and how many duplicates there are
# # Not sure if there's an easier way to do this
# delisting_duplicates = delisting_duplicates %>%
#   group_by(PX_ID) %>%
#   rowwise() %>%
#   mutate(
#     week = case_when(
#       week %in% c(1, 2) ~ list(c(seq(1 + max(week), week + max(duplicates_after_delisting)))),
#       week == 3 & duplicates_after_delisting %in% c(1, 2, 3) ~ list(rep(seq(max(week) + 1, max(week) + max(duplicates_after_delisting)), each = 2)),
#       week == 3 & duplicates_after_delisting == 4 ~ list(c(rep(seq(max(week) + 1, max(week) + 3), each = 2), max(duplicates_after_delisting) + max(week))),
#       week == 3 & duplicates_after_delisting == 5 ~ list(c(rep(seq(max(week) + 1, max(week) + 3), each = 2), max(duplicates_after_delisting) + max(week) - 1, max(duplicates_after_delisting) + max(week))),
#       week >= 4 & week %% 2 == 0 & duplicates_after_delisting %in% c(1,2) ~ list(rep(seq(max(week) + 1, max(week) + max(duplicates_after_delisting)), each = 2)),
#       week > 4 & week %% 2 == 0 & duplicates_after_delisting == 3 ~ list(c(rep(seq(max(week) + 1, max(week) + 2), each = 2), max(duplicates_after_delisting) + max(week))),
#       week > 4 & week %% 2 == 0 & duplicates_after_delisting == 4 ~ list(c(rep(seq(max(week) + 1, max(week) + 2), each = 2), max(duplicates_after_delisting) + max(week) - 1, max(duplicates_after_delisting) + max(week))),
#       week > 4 & week %% 2 != 0 & duplicates_after_delisting == 1 ~ list(rep(max(week) + 1, 3)),
#       week > 4 & week %% 2 != 0 & duplicates_after_delisting %in% c(2,3) ~ list(c(rep(max(week) + 1, 3), rep(seq(max(week) + 2, max(week) + max(duplicates_after_delisting)), 2))),
#       week > 4 & week %% 2 != 0 & duplicates_after_delisting == 4 ~ list(c(rep(max(week) + 1, 3), rep(seq(max(week) + 2, max(week) + max(duplicates_after_delisting - 1)),2), max(week) + max(duplicates_after_delisting))),
#       week > 4 & week %% 2 != 0 & duplicates_after_delisting == 5 ~ list(c(rep(max(week) + 1, 3), rep(seq(max(week) + 2, max(week) + max(duplicates_after_delisting - 2)),2), max(week) + max(duplicates_after_delisting) - 1, max(week) + max(duplicates_after_delisting))),
#       
#       TRUE ~ list(NA_real_)
#     )
#   ) %>% group_by(PX_ID) %>%
#   filter(row_number() == 1) %>%
#   unnest(cols = c(week)) %>%
#   ungroup()


```


## Define landmark variable as function of PX_ID, week, and row_number
## Remove transplant observations
```{r}

combined_df = train_data %>% group_by(PX_ID, week) %>%
  mutate(rown = row_number()) %>% ungroup() %>%
  mutate(landmark = case_when(
    week %in% c(1,2) ~ 1,
    rown == 1 & week %in% c(3,4,5,6) ~ 1,
    rown == 1 & !(week %in% c(1,2,3,4,5,6)) ~ ceiling(week/2) - 2,
    rown == 2 & week %in% c(3,4,5,6) ~ 2,
    rown == 2 & !(week %in% c(3,4,5,6)) ~ ceiling(week/2) - 1,
    rown == 3 & week %in% c(5,6) ~ 3,
    rown == 3 ~ ceiling(week/2)
  )) %>% arrange(PX_ID, landmark) %>% 
  relocate(landmark, .after = CAN_LISTING_CTR_CD) %>%
  select(-rown) %>%
  # remove landmarks where first row delisting2 > 1
  group_by(PX_ID,landmark) %>%
  mutate(rem=first(delisting2)>1) %>% ungroup() %>% filter(!rem) %>% select(-rem)
combined_df %>% filter(PX_ID==1251148) %>% select(PX_ID,landmark,week,interval_start,interval_stop,transplant,outcome,delisting,delisting2,prob_no_tx) %>% View()



# # Adjust outcome variable so it's only 1 in the last week of each patient who dies
# combined_df = combined_df %>% ungroup() %>% 
#   group_by(PX_ID) %>%
#   fill(time_until_death_after_delisting, time_until_censor_after_delisting, .direction = 'down') %>%
#   group_by(PX_ID, landmark) %>%
#   mutate(
#     outcome = case_when(
#       outcome == 1 & row_number() == n() & CAN_REM_CD != 4 ~ 1,
#       week == max(week) & week %% 2 == 0 & !is.na(time_until_death_after_delisting) & time_until_death_after_delisting <= 28 ~ 1,
#       week == max(week) & week %% 2 != 0 & !is.na(time_until_death_after_delisting) & time_until_death_after_delisting <= 35 ~ 1,
#       TRUE ~ 0
#     )
#   )

# is transplant always the last week? yes
combined_df %>% group_by(PX_ID,landmark) %>% filter(transplant==1,row_number()!=n())

# Create weight variable based on prob_no_tx
combined_df = combined_df %>% group_by(PX_ID, landmark) %>% mutate(
  weight = 1/(cumprod(prob_no_tx))
) %>% ungroup()

# save current version
combined_df_tx = combined_df

# remove transplant weeks
combined_df = combined_df %>% filter(transplant == 0)



```

```{r}
combined_df %>% group_by(PX_ID) %>% select(PX_ID, landmark, week, prob_no_tx, weight) 
```

## Landmark start variables
```{r}


combined_df = combined_df %>% group_by(PX_ID, landmark) %>% mutate(
  albumin_at_start = first(albumin),
  bilirubin_at_start = first(bilirubin),
  eGFR_at_start = first(eGFR),
  sodium_at_start = first(sodium),
  BNP_at_start = first(BNP),
  BNP_NT_Pro_at_start = first(BNP_NT_Pro),
  durable_LVAD_at_start = first(durable_LVAD),
  short_mcs_ever_at_start = first(short_MCS_ever),
  status_at_start = first(status),
  cpo_at_start = first(cpo),
  api_at_start = first(api),
  papi_at_start = first(papi)
)
```

## Overall distribution of weights
```{r}
summary(combined_df$weight)

# some people missing weights; these are people with weird statuses?
combined_df %>% filter(is.na(weight)) %>%
  select(albumin,bilirubin,eGFR,sodium,BNP,BNP_NT_Pro,durable_LVAD,short_MCS_ever,status,cpo,api,papi) %>%
  ungroup() %>%
  distinct(PX_ID)
```

## Weight distributions by week within landmark
```{r}
combined_df = combined_df %>% mutate(
  week_in_landmark = row_number()
) %>% ungroup()

combined_df %>% select(PX_ID, landmark, week, week_in_landmark)

with(combined_df,tapply(weight,week_in_landmark,summary))

# distribution_1 = subset(combined_df, week_in_landmark == 1)
# distribution_2 = subset(combined_df, week_in_landmark == 2)
# distribution_3 = subset(combined_df, week_in_landmark == 3)
# distribution_4 = subset(combined_df, week_in_landmark == 4)
# distribution_5 = subset(combined_df, week_in_landmark == 5)
# distribution_6 = subset(combined_df, week_in_landmark == 6)
# 
# summary(distribution_1$weight)
# summary(distribution_2$weight)
# summary(distribution_3$weight)
# summary(distribution_4$weight)
# summary(distribution_5$weight)
# summary(distribution_6$weight)

# how many rows w/ weights above 10 for each week?
with(combined_df,tapply(weight,week_in_landmark,function(x) table(x>10)))



```

## Distribution of landmarks
```{r}
# give some indication of which landmarks are contributing most strongly to estimates
## effectively averaging over this distribution

library(ggplot2)
ggplot(combined_df %>% distinct(PX_ID,landmark),aes(2*(landmark-1))) +
  geom_bar() +
  scale_x_continuous(breaks=seq(0,300,50)) +
  labs(x="Landmark start point (weeks from listing)",
       y="Sample size") +
  theme_bw()
```

## Mortality model without weights
```{r}

mortality_model_no_weights = glm(formula = as.formula(
  outcome ~ 
    albumin_at_start + log(bilirubin_at_start + 1) + eGFR_at_start + log(BNP_at_start + 1):as.character(BNP_NT_Pro_at_start) + as.character(BNP_NT_Pro_at_start) + sodium_at_start + durable_LVAD_at_start + short_mcs_ever_at_start + cpo_at_start + api_at_start + papi_at_start), data = combined_df, family = binomial)


summary(mortality_model_no_weights)
```

```{r}
# lvad test variable for turning lvad variable into "currently qualifying by lvad"
df_intervals = df_intervals %>% dplyr::mutate(
  durable_LVAD = as.numeric(as.character(durable_LVAD)),
  durable_LVAD_test = case_when(
    durable_LVAD == 1 ~ 1,
    lag(PX_ID) == PX_ID & cumsum(durable_LVAD) > 0 ~ 1,
    TRUE ~ 0
  )
) 
             
df_intervals %>% select(PX_ID, durable_LVAD) %>% filter(PX_ID == 1251088)


uscrs_model = glm(formula = as.formula(
  death6week ~ 
    albumin + log(bilirubin + 1) + 
    eGFR + sodium + 
    (log(BNP + 1):BNP_NT_Pro) + 
    durable_LVAD_test + short_MCS_ever), 
  data = df_intervals[df_intervals$train_test == 'Train',], 
  family = binomial)

uscrs_model


```

## Mortality model with weights
```{r}
mortality_model_weights = glm(formula = as.formula(
  outcome ~ 
    albumin_at_start + log(bilirubin_at_start + 1) + eGFR_at_start + log(BNP_at_start + 1):as.character(BNP_NT_Pro_at_start) + as.character(BNP_NT_Pro_at_start) + sodium_at_start + durable_LVAD_at_start + short_mcs_ever_at_start + cpo_at_start + api_at_start + papi_at_start), data = combined_df, weights = weight, family = binomial)


mortality_model_weights
```



## Check distribution by mcs variable
```{r}
short_mcs = combined_df %>% filter(short_MCS_ever == 1)
no_short_mcs = combined_df %>% filter(short_MCS_ever == 0)

summary(short_mcs$weight)
summary(no_short_mcs$weight)
```
## Truncate weights and rerun model
```{r}
combined_df = combined_df %>% mutate(
  weight = ifelse(weight > 10, 10, weight)
)

mortality_model_weights = glm(formula = as.formula(
  outcome ~ 
    albumin_at_start + log(bilirubin_at_start + 1) + eGFR_at_start + log(BNP_at_start + 1):as.character(BNP_NT_Pro_at_start) + as.character(BNP_NT_Pro_at_start) + sodium_at_start + durable_LVAD_at_start + short_mcs_ever_at_start + cpo_at_start + api_at_start + papi_at_start), data = combined_df, weights = weight, family = binomial)


mortality_model_weights
```

```{r}
# Extract coefficients and confidence intervals
coef1 <- summary(mortality_model_no_weights)$coefficients
coef2 <- summary(uscrs_model)$coefficients


# Create data frames with the coefficients and confidence intervals
df1 <- data.frame(
  term = rownames(coef1),
  estimate = coef1[, "Estimate"],
  conf.low = coef1[, "Estimate"] - 1.96 * coef1[, "Std. Error"],
  conf.high = coef1[, "Estimate"] + 1.96 * coef1[, "Std. Error"],
  model = "Model without Weights"
)

# Same terms as US-CRS + hemodynamics
df2 <- data.frame(
  term = setdiff(rownames(coef1), c("papi_at_start", "cpo_at_start", "api_at_start")),
  estimate = coef2[, "Estimate"],
  conf.low = coef2[, "Estimate"] - 1.96 * coef2[, "Std. Error"],
  conf.high = coef2[, "Estimate"] + 1.96 * coef2[, "Std. Error"],
  model = "USCRS 1.0"
)

df1 <- df1[df1$term != "(Intercept)", ]
df2 <- df2[df2$term != "(Intercept)", ]

# Combine the two data frames
forest_df <- rbind(df1, df2)

```

## Forest plot
```{r}

# Rename and reorder variables
new_names <- c("sodium_at_start" = "Sodium", "short_mcs_ever_at_start" = "Short-term MCS", "papi_at_start" = "PAPI", "log(BNP_at_start + 1):as.character(BNP_NT_Pro_at_start)0" = "Log BNP, Regular BNP", "log(BNP_at_start + 1):as.character(BNP_NT_Pro_at_start)1" = "Log BNP, NT-proBNP", "log(bilirubin_at_start + 1)" = "Log Bilirubin", "eGFR_at_start" = "eGFR", "durable_LVAD_at_start" = "Durable LVAD", "cpo_at_start" = "CPO", "api_at_start" = "API", "albumin_at_start" = "Albumin" )
term_order <- c("PAPI", "API", "CPO", "Durable LVAD", "Sodium", "Albumin", "Log BNP, NT-proBNP", "Log BNP, Regular BNP", "eGFR", "Log Bilirubin", "Short-term MCS")
forest_df

forest_df <- forest_df %>%
  mutate(term = recode(term, !!!new_names))
forest_df <- forest_df %>%
  mutate(term = factor(term, levels = term_order))# Create the forest plot
ggplot(forest_df, aes(x = estimate, y = term, color = model)) +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2, position = position_dodge(width = 0.5)) +
  labs(x = "Coefficient Estimate", y = "Term", title = "Model Coefficients") +
  theme_minimal()

new_names
```




